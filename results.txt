Results for MultiWoZ 2.0 NLG

SC-LSTM ........
Calculating BLEU...
Avg # feat: 622
Avg # gen: 49.39

Done BLEU-4, time:234.4
BLEU 1-4: [0.6400023268232907]
BLEU 1-4: [0.6400023268232907]

MLE baseline LSTM no pointer GEN
1000 example in 142 sec
2000 example in 134 sec
3000 example in 146 sec
Average BLEU score: 0.30761174281728504
juchuyun woz_mle_base

1000 example in 71 sec
2000 example in 70 sec
3000 example in 72 sec
Average BLEU score: 0.32301135165267864




RL Pretrain with pointer gen
on cloud
1000 example in 334 sec
2000 example in 332 sec
3000 example in 342 sec
Average BLEU score: 7.452423028969202e-157

pretrain dagger_pretrain

xo 20000 iteration

0.11616499478045068

pretrain mixer_RL
xo :~/Desktop/mixerDagger_new/R250Project/src$ python decode.py ../mixer_rl/best_model/model_best_15000
Average BLEU score: 0.0967636309591495

pretrain mixer_Dagger
xo :~/Desktop/mixerDagger/R250Project/src$ python decode.py ../mixer_rl/best_model/model_best_20000
Average BLEU score: 0.1367916902034269



MLE pretrain, + RL fintune
Average BLEU score: 4.5168102313131205e-05
doesn't work.
rerun https://hz.matpool.com:29570/lab/workspaces/auto-T

Average BLEU score: 0.2274416273586806

Average BLEU score: 0.32615020496000785


MLE pretrain, + Dagger fintune
---30000
ssh -p 26168 root@hz.matpool.com
Average BLEU score: 0.2686148770537568
1000 example in 72 sec
2000 example in 72 sec
3000 example in 74 sec
Average BLEU score: 0.31182387049828214



MLE pretrain, + mixer RL fintune

Average BLEU score: 0.3153887562386047

MLE pretrain, + mixer Dagger fintune
running cloud
ssh -p 28063 root@hz.matpool.com
# Parameter for the mixer
isMixer = True
mixer_delta = 1
mixer_T =  40
mixer_N_XENT_step = 1000
mixer_N_XENTRL_step = 200
not_normalise_reward = False

1000 example in 90 sec
2000 example in 88 sec
3000 example in 91 sec
Average BLEU score: 0.2681886504962757

running https://hz.matpool.com:29570/lab/workspaces/auto-T with old code.

1000 example in 72 sec
2000 example in 68 sec
3000 example in 68 sec
Average BLEU score: 0.3153887562386047


isMixer = True
mixer_delta = 2
mixer_T =  40
mixer_N_XENT_step = 2000
mixer_N_XENTRL_step = 1000
not_normalise_reward = False

  warnings.warn(_msg)
1000 example in 88 sec
2000 example in 86 sec
3000 example in 88 sec
Average BLEU score: 0.2684747271482392

https://hz.matpool.com:28381/lab run new without pointer gen.
oading WoZ, loading test
1000 example in 71 sec
2000 example in 68 sec
3000 example in 73 sec
Average BLEU score: 0.3178774660043566






Results for DSTC9

LSTM DSTC 9 pointer gen MLE
rerunning on https://hz.matpool.com:26804/lab
1000 example in 104 sec
Average BLEU score: 0.21046169875827023

juchiyun dstc_MLE

LSTM DSTC 9 pointer gen RL
runing on xo
2.13777e-15
not work.


LSTM DSTC 9 pointer gen Dagger
https://hz.matpool.com:29095/lab

LSTM DSTC 9 pointer gen mixer rl pretrain
https://hz.matpool.com:28808/lab
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
1000 example in 182 sec
Average BLEU score: 0.07105854631488437

LSTM DSTC 9 pointer gen mixer dagger pretrain
https://hz.matpool.com:26325/lab
