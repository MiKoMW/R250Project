Results for MultiWoZ 2.0 NLG

SC-LSTM ........
Calculating BLEU...
Avg # feat: 622
Avg # gen: 49.39

Done BLEU-4, time:234.4
BLEU 1-4: [0.6400023268232907]
BLEU 1-4: [0.6400023268232907]

MLE baseline LSTM no pointer GEN
1000 example in 142 sec
2000 example in 134 sec
3000 example in 146 sec
Average BLEU score: 0.30761174281728504
juchuyun woz_mle_base


RL Pretrain with pointer gen
on cloud
1000 example in 334 sec
2000 example in 332 sec
3000 example in 342 sec
Average BLEU score: 7.452423028969202e-157

pretrain dagger_pretrain

xo 20000 iteration

0.11616499478045068

pretrain mixer_RL
xo :~/Desktop/mixerDagger_new/R250Project/src$ python decode.py ../mixer_rl/best_model/model_best_15000
Average BLEU score: 0.0967636309591495


pretrain mixer_Dagger
xo :~/Desktop/mixerDagger/R250Project/src$ python decode.py ../mixer_rl/best_model/model_best_20000
Average BLEU score: 0.1367916902034269

MLE pretrain, + RL fintune
Average BLEU score: 4.5168102313131205e-05
doesn't work.

MLE pretrain, + Dagger fintune
---30000
ssh -p 26168 root@hz.matpool.com
Average BLEU score: 0.2686148770537568

MLE pretrain, + mixer RL fintune
Average BLEU score: 0.3153887562386047


MLE pretrain, + mixer Dagger fintune
running cloud
ssh -p 28063 root@hz.matpool.com
# Parameter for the mixer
isMixer = True
mixer_delta = 1
mixer_T =  40
mixer_N_XENT_step = 1000
mixer_N_XENTRL_step = 200
not_normalise_reward = False



Results for DSTC9

LSTM DSTC 9 pointer gen MLE
rerunning on https://hz.matpool.com:26804/lab
1000 example in 104 sec
Average BLEU score: 0.21046169875827023

juchiyun dstc_MLE





LSTM DSTC 9 pointer gen RL
runing on xo

LSTM DSTC 9 pointer gen Dagger
https://hz.matpool.com:29095/lab

LSTM DSTC 9 pointer gen mixer rl pretrain
https://hz.matpool.com:28808/lab


LSTM DSTC 9 pointer gen mixer dagger pretrain
https://hz.matpool.com:26325/lab
