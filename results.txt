Results for MultiWoZ 2.0 NLG

SC-LSTM ........
Calculating BLEU...
Avg # feat: 622
Avg # gen: 49.39

Done BLEU-4, time:234.4
BLEU 1-4: [0.6400023268232907]
BLEU 1-4: [0.6400023268232907]

MLE baseline LSTM no pointer GEN
1000 example in 142 sec
2000 example in 134 sec
3000 example in 146 sec
Average BLEU score: 0.30761174281728504
juchuyun woz_mle_base



pretrain dagger_pretrain
xo running

1000 example in 115 sec
2000 example in 115 sec
3000 example in 115 sec
Average BLEU score: 0.0967636309591495




pretrain mixer_RL
xo :~/Desktop/mixerDagger_new/R250Project/src$ python decode.py ../mixer_rl/best_model/model_best_15000

1000 example in 115 sec
2000 example in 115 sec
3000 example in 115 sec
Average BLEU score: 0.0967636309591495


pretrain mixer_Dagger
xo :~/Desktop/mixerDagger/R250Project/src$ python decode.py ../mixer_rl/best_model/model_best_20000

1000 example in 115 sec
2000 example in 115 sec
3000 example in 115 sec
Average BLEU score: 0.1367916902034269



MLE pretrain, + mixer RL fintune
1000 example in 99 sec
2000 example in 97 sec
3000 example in 99 sec
Average BLEU score: 0.3153887562386047


RL Pretrain with pointer gen
running on cloud 28808


Results for DSTC9

LSTM DSTC 9 pointer gen MLE

1000 example in 104 sec
Average BLEU score: 0.21046169875827023

juchiyun dstc_MLE

